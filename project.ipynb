{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr9kA_lBnA3l",
        "outputId": "fc2ae528-94ac-464b-a582-f77d0b3be969"
      },
      "outputs": [],
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import cm\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "from retry import retry\n",
        "\n",
        "import string\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk import download\n",
        "download('punkt')\n",
        "download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk import Text as nltk_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM7FfXJ2nA3m"
      },
      "outputs": [],
      "source": [
        "about_url = 'https://bigbangtrans.wordpress.com/about/'\n",
        "\n",
        "page = rq.get(about_url)\n",
        "soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    if link.parent.name == 'li':\n",
        "        links.append(link.get('href'))\n",
        "\n",
        "episodes = []\n",
        "for link in links:\n",
        "    if 'episode' in link:\n",
        "        episodes.append(link)\n",
        "episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xMxs20InA3o"
      },
      "outputs": [],
      "source": [
        "@retry()\n",
        "def parser(num):\n",
        "    episode = rq.get(episodes[num])\n",
        "    soup = BeautifulSoup(episode.text, features=\"html.parser\")\n",
        "    episode_name = soup.find('h2').text\n",
        "    season_number = episode_name[7:9]\n",
        "    episode_number = episode_name[18:20]\n",
        "    text = []\n",
        "    for k in soup.find_all('p'):\n",
        "        text.append(k.text)\n",
        "    episode_text = ' '.join(text)\n",
        "    episode_text = episode_text.replace('\\xa0', ' ')\n",
        "    return season_number, episode_number, episode_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2iZMHknnA3o"
      },
      "outputs": [],
      "source": [
        "scripts = []\n",
        "for num in range(len(episodes)):\n",
        "    one_script = parser(num)\n",
        "    scripts.append(one_script)\n",
        "\n",
        "df = pd.DataFrame(scripts)\n",
        "df.head()\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.columns = ['season', 'episode', 'script']\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECqX8anDnA3o"
      },
      "outputs": [],
      "source": [
        "df.to_csv('TBBT.csv', sep = ',', encoding='utf-8')\n",
        "df.to_excel('TBBT excel.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcJ0vRP3nA3p"
      },
      "source": [
        "#запускать отсюда"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W0XrHb-nA3q"
      },
      "outputs": [],
      "source": [
        "#стоп-слова - имена персонажей\n",
        "characters_full_names = [\n",
        "    'Leonard Hofstadter',\n",
        "    'Sheldon Cooper',\n",
        "    'Penny',\n",
        "    'Howard Wolowitz',\n",
        "    'Raj Koothrappali',\n",
        "    'Bernadette Rostenkowski Wolowitz',\n",
        "    'Amy Farrah Fowler',\n",
        "    'Stuart Bloom'\n",
        "    ]\n",
        "\n",
        "clean_names = []\n",
        "for name in characters_full_names:\n",
        "    name = name.lower().split()\n",
        "    name_clean = [word for word in name]\n",
        "    clean_names.append(name_clean)\n",
        "\n",
        "characters_stop_names = []\n",
        "for full_name in clean_names:\n",
        "    for item in full_name:\n",
        "        characters_stop_names.append(item)\n",
        "characters_stop_names.remove('bloom')\n",
        "\n",
        "#стоп-слова - частотные слова из сериала, которые я не посчитала информативными\n",
        "my_stop_words = []\n",
        "with open('my stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "  for i in f:\n",
        "    my_stop_words.append(i.strip())\n",
        "\n",
        "#стоп-слова - 50 самых распространенных глаголов в английском\n",
        "#источник: https://www.ef.com/wwen/english-resources/english-vocabulary/top-50-verbs/\n",
        "#я немного изменила этот список\n",
        "common_verbs = []\n",
        "with open('50 most common verbs in English.txt', 'r', encoding='utf-8') as f:\n",
        "  for i in f:\n",
        "    common_verbs.append(i.strip())\n",
        "\n",
        "#собираем все стоп-слова\n",
        "stop_words = stopwords.words('english') + characters_stop_names + my_stop_words + common_verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSK90_zGnA3q"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    text = text.replace('Create a free website or blog at WordPress.com', '')\n",
        "    text = text.replace('Blog at WordPress.com', '')\n",
        "    text = text.lower()\n",
        "    text_clean = word_tokenize(text)\n",
        "    text_clean = [word for word in text_clean if word[0].isalpha()]\n",
        "    text_clean = [word for word in text_clean if word not in stop_words]\n",
        "    return text_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVxT9gfTnA3r"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('TBBT.csv')\n",
        "clean_script = []\n",
        "script = data['script'].to_list()\n",
        "\n",
        "for episode in script:\n",
        "    clean_episode = clean(episode)\n",
        "    clean_script.append(clean_episode)\n",
        "\n",
        "data['clean script'] = clean_script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke2BUFXvnA3r",
        "outputId": "1fed4c6d-ed12-4f5f-f67d-d4b85fcc2007"
      },
      "outputs": [],
      "source": [
        "#пример для того, чтобы убедиться, что все получилось\n",
        "res = data['clean script'].to_list()\n",
        "print(res[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q63RDxQznA3r"
      },
      "outputs": [],
      "source": [
        "#не запускать\n",
        "# nltk без частеречной разметки\n",
        "\n",
        "lemmatized = []\n",
        "not_lemmatized = data['clean script'].to_list()\n",
        "\n",
        "for ep in not_lemmatized:\n",
        "    lemmatized_episode = []\n",
        "    for word in ep:\n",
        "        lemmatized_word = lemmatizer.lemmatize(word)\n",
        "        lemmatized_episode.append(lemmatized_word)\n",
        "    lemmatized.append(lemmatized_episode)\n",
        "lemmatized_result = [word for word in lemmatized if word not in stop_words]\n",
        "#data['nltk lemmatized script'] = lemmatized_result\n",
        "\n",
        "#lemmatized_result[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJT5xozDnA3r"
      },
      "outputs": [],
      "source": [
        "#nltk с частеречной разметкой - объявление функций\n",
        "def pos_tagger(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    res = []\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for word, pos in pos_tag(word_tokenize(text)):\n",
        "        wordnet_pos = pos_tagger(pos) or wordnet.NOUN\n",
        "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG-xMVwQnA3r",
        "outputId": "aee9d4d2-ed54-4cc8-98a9-f527c7c8c03f"
      },
      "outputs": [],
      "source": [
        "#nltk с частеречной разметкой - лемматизация\n",
        "#каждое слово лемматизируется отдельно и добавляется в список-эпизод\n",
        "lemmatized_tagged = []\n",
        "not_lemmatized = data['clean script'].to_list()\n",
        "\n",
        "for ep in not_lemmatized:\n",
        "    lemmatized_episode_tagged = []\n",
        "    for word in ep:\n",
        "        lemmatized_episode_tagged += lemmatize_text(word)\n",
        "    lemmatized_tagged.append(lemmatized_episode_tagged)\n",
        "\n",
        "#lemmatized_tagged[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-zYt99LnA3s"
      },
      "outputs": [],
      "source": [
        "lemmatized_tagged_result = []\n",
        "for l in lemmatized_tagged:\n",
        "    res = []\n",
        "    for word in l:\n",
        "        if word not in stop_words:\n",
        "            res.append(word)\n",
        "    lemmatized_tagged_result.append(res)\n",
        "\n",
        "data['nltk lemmatized script (with pos tagging)'] = lemmatized_tagged_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPzkY1AWnA3s"
      },
      "outputs": [],
      "source": [
        "#не запускать\n",
        "#с этим кодом не удаляются стоп-слова\n",
        "lemmatized_tagged_result = [word for word in lemmatized_tagged if word not in stop_words]\n",
        "data['nltk lemmatized script (with pos tagging)'] = lemmatized_tagged_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "K6PgbW1lnA3s",
        "outputId": "a55336ef-83aa-42be-d55e-43e04a5d5a68"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEux7KWknA3s",
        "outputId": "50f13b8f-52ca-4f07-ce9e-c83a5236fa94"
      },
      "outputs": [],
      "source": [
        "#пример для того, чтобы убедиться, что все получилось\n",
        "res = data['nltk lemmatized script (with pos tagging)'].to_list()\n",
        "print(res[207])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqzf8-vznA3s",
        "outputId": "e633aea0-483b-49e7-9b33-b0fad2bbe34c"
      },
      "outputs": [],
      "source": [
        "#частотные слова во всем сериале\n",
        "nltk_lemmatized_script = data['nltk lemmatized script (with pos tagging)'].to_list()\n",
        "nltk_full_script = []\n",
        "for ep in nltk_lemmatized_script:\n",
        "    for list in ep:\n",
        "        nltk_full_script.append(list)\n",
        "counter_series = Counter(nltk_full_script)\n",
        "counter_series_100 = Counter(nltk_full_script).most_common(100)\n",
        "\n",
        "#print(counter_series_100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk6xdtFtnA3t"
      },
      "outputs": [],
      "source": [
        "#частотные слова в каждом эпизоде\n",
        "counter_episode = []\n",
        "counter_episode_10 = []\n",
        "for ep in nltk_lemmatized_script:\n",
        "    counter_episode.append(Counter(ep))\n",
        "    counter_episode_10.append(Counter(ep).most_common(10))\n",
        "\n",
        "data['counter'] = counter_episode\n",
        "data['10 most common words'] = counter_episode_10\n",
        "\n",
        "#print(counter_episode_10[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDL5uw5rnA3t"
      },
      "outputs": [],
      "source": [
        "#биграммы во всем сериале\n",
        "bigrams_series = Counter(nltk.bigrams(nltk_full_script))\n",
        "bigrams_series_50 = bigrams_series.most_common(50)\n",
        "\n",
        "#bigrams_series\n",
        "#bigrams_series_50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZINiVsDnA3t"
      },
      "outputs": [],
      "source": [
        "#биграммы в каждой серии\n",
        "bigrams_episode = []\n",
        "bigrams_episode_10 = []\n",
        "for ep in nltk_lemmatized_script:\n",
        "    bigrams_episode.append(Counter(nltk.bigrams(ep)))\n",
        "    bigrams_episode_10.append(Counter(nltk.bigrams(ep)).most_common(10))\n",
        "data['all bigrams'] = bigrams_episode\n",
        "data['5 most common bigrams'] = bigrams_episode_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#выявление частотных слов и биграмм отдельно в сезонах 1-3 и 4-10\n",
        "seasons_1_3 = data[:63]\n",
        "seasons_4_10 = data[63:]\n",
        "\n",
        "script_1_3 = []\n",
        "for ep in nltk_lemmatized_script[:63]:\n",
        "    for list in ep:\n",
        "        script_1_3.append(list)\n",
        "\n",
        "script_4_10 = []\n",
        "for ep in nltk_lemmatized_script[63:]:\n",
        "    for list in ep:\n",
        "        script_4_10.append(list)\n",
        "\n",
        "counter_1_3 = Counter(script_1_3)\n",
        "counter_1_3_100 = counter_1_3.most_common(100)\n",
        "\n",
        "counter_4_10 = Counter(script_4_10)\n",
        "counter_4_10_100 = counter_4_10.most_common(100)\n",
        "\n",
        "bigrams_1_3 = Counter(nltk.bigrams(script_1_3))\n",
        "bigrams_1_3_50 = bigrams_1_3.most_common(50)\n",
        "\n",
        "bigrams_4_10 = Counter(nltk.bigrams(script_4_10))\n",
        "bigrams_4_10_50 = bigrams_4_10.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#проверяем, есть ли слова в списке 100 частотных слов сезонов 1-3\n",
        "def check(text):\n",
        "    flag = False\n",
        "    for word in counter_1_3_100:\n",
        "        for i in word:\n",
        "            if i == text:\n",
        "                flag = True\n",
        "    return flag\n",
        "\n",
        "check(input())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#начисление очков за совпадение частотных слов и биграмм сезонов 4-10 со словами и биграммами сезонов 1-3\n",
        "#считается для всех серий для упрощения упорядочивания данных\n",
        "counter_4_10_points = []\n",
        "for ep in counter_episode_10:\n",
        "    score = 0\n",
        "    for word in ep:\n",
        "      for pair in counter_1_3_100:\n",
        "        if word[0] == pair[0]:\n",
        "            score += 1\n",
        "    counter_4_10_points.append(score)\n",
        "\n",
        "\n",
        "bigrams_4_10_points = []\n",
        "for ep in bigrams_episode_10:\n",
        "    score = 0\n",
        "    for bigram in ep:\n",
        "      for pair in bigrams_1_3_50:\n",
        "        if bigram[0] == pair[0]:\n",
        "            score += 1\n",
        "    bigrams_4_10_points.append(score)\n",
        "\n",
        "data['points for words from seasons 1-3'] = counter_4_10_points\n",
        "data['points for bigrams from seasons 1-3'] = bigrams_4_10_points\n",
        "\n",
        "#counter_4_10_points[63:]\n",
        "#bigrams_4_10_points[63:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#среднее значение\n",
        "print(np.average(counter_4_10_points[63:]))\n",
        "print(np.average(bigrams_4_10_points[63:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#график с частотными биграммами всех сезонов\n",
        "plt.plot(data['points for bigrams from seasons 1-3'])\n",
        "\n",
        "#plt.savefig('points_for_common_bigrams.png', bbox_inches='tight')\n",
        "\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#график с частотными биграммами только сезонов 4-10\n",
        "plt.plot(data['points for bigrams from seasons 1-3'][63:])\n",
        "\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#график с частотными словами всех сезонов\n",
        "plt.plot(data['points for words from seasons 1-3'])\n",
        "\n",
        "#plt.savefig('points_for_common_words.png', bbox_inches='tight')\n",
        "\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#график с частотными словами только сезонов 4-10\n",
        "plt.plot(data['points for words from seasons 1-3'][63:])\n",
        "\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**практическое применение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaU9I1ZlwooI",
        "outputId": "881dabcf-ac20-41f6-f760-852a60ad2d78"
      },
      "outputs": [],
      "source": [
        "#в результате серии, где встречается хотя бы одно слово из запроса\n",
        "search = input('Введите слова без запятых:').split()\n",
        "season = data['season'].to_list()\n",
        "episode = data['episode'].to_list()\n",
        "for ep in range(len(nltk_lemmatized_script)):\n",
        "    for word in search:\n",
        "        if word in nltk_lemmatized_script[ep]:\n",
        "            print(season[ep], episode[ep], word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#в результате только серии, где встречаются все слова из запроса\n",
        "search = input('Введите слова без запятых:').split()\n",
        "season = data['season'].to_list()\n",
        "episode = data['episode'].to_list()\n",
        "for ep in range(len(nltk_lemmatized_script)):\n",
        "    counter = 0\n",
        "    for word in search:\n",
        "        if word in nltk_lemmatized_script[ep]:\n",
        "            counter += 1\n",
        "    if counter == len(search):\n",
        "        print(season[ep], episode[ep])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#облако слов\n",
        "data['counter'] = counter_episode\n",
        "def get_wordcloud(num):\n",
        "    wordcloud = WordCloud(width = 1500,\n",
        "                      height = 1000,\n",
        "                      include_numbers = False,\n",
        "                      background_color='black',\n",
        "                      colormap='magma').generate(', '.join(counter_episode[num]))\n",
        "    plt.figure(figsize=(30, 20)) \n",
        "    plt.imshow(wordcloud) \n",
        "    plt.axis(\"off\") \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_wordcloud(int(input()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4XZQoTpnA3u",
        "outputId": "40e401b8-2658-4510-924c-dc1fddc30245"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hrc_XiBnA3u"
      },
      "outputs": [],
      "source": [
        "data.to_csv('TBBT with data.csv', sep = ',', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**идеи на будущее**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "соотнесение оценок пользователей и ключевых слов серии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#начисление очков за совпадение частотных слов серии и сериала\n",
        "#для сравнения с оценками пользователей\n",
        "counter_points = []\n",
        "for ep in counter_episode_10:\n",
        "    score = 0\n",
        "    for word in ep:\n",
        "      for pair in counter_series_100:\n",
        "        if word[0] == pair[0]:\n",
        "            score += 1\n",
        "    counter_points.append(score)\n",
        "counter_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#начисление очков за совпадение частотных биграм серии и сериала\n",
        "#для сравнения с оценками пользователей\n",
        "bigrams_points = []\n",
        "for ep in bigrams_episode_10:\n",
        "    score = 0\n",
        "    for bigram in ep:\n",
        "      for pair in bigrams_series_50:\n",
        "        if bigram[0] == pair[0]:\n",
        "            score += 1\n",
        "    bigrams_points.append(score)\n",
        "bigrams_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufOEEucpnA3u"
      },
      "source": [
        "тематическое моделирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWtgi_YOnA3u"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from collections import defaultdict\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim import models\n",
        "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel\n",
        "from gensim import corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx-tn2nunA3u"
      },
      "outputs": [],
      "source": [
        "texts = data['nltk lemmatized script (with pos tagging)'].to_list()\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
        "corpus_lsi = lsi_model[corpus_tfidf]\n",
        "\n",
        "lsi_model.print_topics(5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
